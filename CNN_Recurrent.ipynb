{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN_Recurrent.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ysOvT6g9Jb6U"},"source":["# CNNs for Heart Rate Estimation and Human Activity Recognition in Wrist Worn Sensing Applications"]},{"cell_type":"markdown","metadata":{"id":"ZR_fmQU4epdk","colab_type":"text"},"source":["This is code for reproducing the CNNR HRE results shown in the paper presented at the WristSense workshop as part of PerCom 2020.\n","\n","This repository will be broken down as shown in the Figure 1 below."]},{"cell_type":"markdown","metadata":{"id":"JMQwpmznepdl","colab_type":"text"},"source":["![](./Block_Diagram_LS.png)\n","\n","Figure 1. *Block diagram of our processing approach*"]},{"cell_type":"markdown","metadata":{"id":"tJZicqMzepdm","colab_type":"text"},"source":["## Data Collection"]},{"cell_type":"markdown","metadata":{"id":"eDyI4gZNepdo","colab_type":"text"},"source":["The data was collected by [D. Jarchi and A. Casson (2017)](https://www.mdpi.com/2306-5729/2/1/1) and downloaded from [PhysioNet](https://physionet.org/content/wrist/1.0.0/)."]},{"cell_type":"markdown","metadata":{"id":"NE_5xa1Eepdp","colab_type":"text"},"source":["### If using Google Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"NmrsuF8Mepdp","colab_type":"text"},"source":["You can run this notebook on Colab using the following cell to mount your drive and install some dependencies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gkit0T_9JSj4","colab":{}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","# Pull the data from PhysioNet - Specify a Path\n","#!gsutil -m cp -r gs://wrist-1.0.0.physionet.org '/content/drive/My Drive/..../Data'\n","\n","# Install wfdb\n","!pip install wfdb\n","\n","# Change cwd if necessary\n","import os\n","path = '/content/drive/My Drive/WristSense_Experiments/Data'\n","os.chdir(path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8pbdZ-elkxT6","colab_type":"text"},"source":["### If your running on your own machine/server\n","You may need to install some of these packages below"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KwZHppRdJlHp","colab":{}},"source":["import wfdb\n","from wfdb import processing\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import json as js\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","torch.manual_seed(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mWQo4nxHJvX6"},"source":["## Load Data\n","\n","This step is done in by selecting each exercise at a time. We can begin with the 'walk' exercise.\n","\n","By changing the word below betwwen 'high', 'low', 'run', 'walk' we can pre-process our data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uOCtRI-sJq10","colab":{}},"source":["def load_data(fileDir, exercise):\n","    word = exercise\n","    file_path_list = []\n","    valid_file_extensions = [\".dat\"]\n","    valid_file_extensions = [item.lower() for item in valid_file_extensions]\n","\n","\n","    for file in os.listdir(fileDir):\n","        extension = os.path.splitext(file)[1]\n","        if extension.lower() not in valid_file_extensions:\n","            continue\n","        file_path_list.append(os.path.join(fileDir, file))\n","\n","    PPG = []\n","    ECG = []\n","    for path in file_path_list:\n","        base=os.path.basename(path)\n","        base = os.path.splitext(base)[0]\n","        if word in base:\n","            sample = wfdb.rdsamp('wrist/%s'%(base))\n","            PPG.append(sample[0][:,1])\n","            ECG.append(sample[0][:,0])\n","\n","    PPG = np.asarray(PPG)\n","    ECG = np.asarray(ECG)\n","\n","    return PPG, ECG"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LPS47Xl3epdz","colab_type":"text"},"source":["## Segment, Normalise and Downsample Data\n","\n","\n","```slidingWindow()``` returns a generator that iterates through the input sequence."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wAsW9HlSJ4PG","colab":{}},"source":["def slidingWindow(sequence,winSize=2048,step=256):\n","    \"\"\"Returns a generator that will iterate through\n","    the defined chunks of input sequence.  Input sequence\n","    must be iterable.\"\"\"\n"," \n","    # Verify the inputs\n","    try: it = iter(sequence)\n","    except TypeError:\n","        raise Exception(\"**ERROR** sequence must be iterable.\")\n","    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n","        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n","    if step > winSize:\n","        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n","    if winSize > len(sequence):\n","        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n"," \n","    # Pre-compute number of chunks to emit\n","    numOfChunks = ((len(sequence)-winSize)//step)+1\n"," \n","    # Do the work\n","    for i in range(0,numOfChunks*step,step):\n","        yield sequence[i:i+winSize]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S4Lh8JOcJ9qX"},"source":["### PPG and ECG\n","\n","Here the PPG signal is:\n","1. Segmented using the ```slidingWindow``` function\n","2. Normalised between 0 and 1 (for each sample) using ```wfdb.processing.normalise_bound```\n","3. Cleaned (Removed any rows with NaNs from segmentation)\n","4. Downsampled using the downsampling factor ```ds_factor```.\n","\n","The returned signal **p** is ready to be used in the experiments\n","\n","The ECG signal is segmented here using the ```slidingWindow``` function and then cleaned as above."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"woyVnSo5J-pH","colab":{}},"source":["def preprocess_PPG_ECG(PPG, ECG, downsample=True, ds_factor=25):\n","    prev_p = []\n","    e = []\n","\n","    for i in range(len(PPG)):\n","        ppg = slidingWindow(PPG[i], winSize = 2048)\n","        for sig in ppg:\n","            nrm_sig = processing.normalize_bound(sig, lb=0, ub=1) # normalise signal\n","            prev_p.append(nrm_sig)\n","\n","    for i in range(len(ECG)):\n","        ecg = slidingWindow(ECG[i], winSize = 2048)\n","        for sig in ecg:\n","            e.append(sig)\n","\n","    prev_p = np.asarray(prev_p, dtype=np.float32)  #290 and 781\n","    e = np.asarray(e, dtype=np.float32)\n","\n","\n","    p = prev_p[~np.isnan(prev_p).any(axis=1)] # Remove rows with NaN\n","    e = e[~np.isnan(prev_p).any(axis=1)] # Remove rows corresponding to PPG NaN\n","\n","    # Don't need to downsample ECG\n","    if downsample == True:\n","        p = p[:,::ds_factor]\n","    print(p.shape)\n","    return p,e"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ogfNGBJ6cerJ"},"source":["### Load Ground Truth of ECG"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sHVEBtpcez6p"},"source":["Get QRS of ECG signal using ```wfdb.processing.XQRS()```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9rDjWSWRchNZ","colab":{}},"source":["def gt_ECG(ecg):\n","    \n","    y = []\n","    for i in range(len(ecg)):\n","        sig = ecg[i, :]\n","        xqrs = processing.XQRS(sig=sig, fs=256.0)\n","        xqrs.detect()\n","\n","        HR = processing.compute_hr(len(sig), xqrs.qrs_inds, 256.0)\n","        HR = HR[np.logical_not(np.isnan(HR))] # Remove any NaN in HR Array\n","        AvgHR = (np.mean(HR))\n","\n","        y.append(AvgHR)\n","\n","    y = np.asarray(y, dtype=np.float32)\n","    y = np.around(y)\n","\n","    max_y = max(y)\n","    y = y/max(y)\n","    print(y.shape)\n","    \n","    return y, max_y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HUK2a9RWKDMG"},"source":["## RCNN Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WdvNsAIsKDT0","colab":{}},"source":["class RCNN(nn.Module):\n","    def __init__(self, input_size, batch_size, n_features, \n","                 cv1_k, cv1_s, cv2_k, cv2_s,\n","                 cv3_k, cv3_s, cv4_k, cv4_s):\n","        super(RCNN, self).__init__()\n","    \n","        self.input_size = input_size\n","    \n","        self.cv1_k = cv1_k\n","        self.cv1_s = cv1_s\n","        self.cv1_out = int(((self.input_size - self.cv1_k)/self.cv1_s) + 1)\n","\n","        self.cv2_k = cv2_k\n","        self.cv2_s = cv2_s\n","        self.cv2_out = int(((self.cv1_out - self.cv2_k)/self.cv2_s) + 1)\n","\n","        self.cv3_k = cv3_k\n","        self.cv3_s = cv3_s\n","        self.cv3_out = int(((self.cv2_out - self.cv3_k)/self.cv3_s) + 1)\n","\n","        self.cv4_k = cv4_k\n","        self.cv4_s = cv4_s\n","        self.cv4_out = int(((self.cv3_out - self.cv4_k)/self.cv4_s) + 1)\n","    \n","        self.layer_1 = nn.Sequential(\n","          nn.Conv1d(in_channels=1, out_channels=3, kernel_size=(self.cv1_k), stride=(self.cv1_s)),\n","          nn.BatchNorm1d(num_features=3),\n","          nn.ReLU(inplace=True),\n","          nn.AvgPool1d(kernel_size=1)\n","        )\n","\n","        self.layer_2 = nn.Sequential(\n","          nn.Conv1d(in_channels=3, out_channels=5, kernel_size=(self.cv2_k), stride=(self.cv2_s)),\n","          nn.BatchNorm1d(num_features=5),\n","          nn.ReLU(inplace=True),\n","          nn.AvgPool1d(kernel_size=1)\n","        )\n","\n","        self.layer_3 = nn.Sequential(\n","          nn.Conv1d(in_channels=5, out_channels=8, kernel_size=(self.cv3_k), stride=(self.cv3_s)),\n","          nn.BatchNorm1d(num_features=8),\n","          nn.ReLU(inplace=True)\n","        )\n","\n","        self.layer_4 = nn.Sequential(\n","          nn.Conv1d(in_channels=8, out_channels=10, kernel_size=(self.cv4_k), stride=(self.cv4_s)),\n","          nn.BatchNorm1d(num_features=10),\n","          nn.ReLU(inplace=True),\n","          nn.Dropout(p=0.5, inplace=False)\n","        )\n","\n","        self.layer_5 = nn.Sequential(\n","          nn.Linear(self.cv4_out*10, 20), # FC Layer\n","          nn.Linear(20, 1) # Regression\n","        )\n","        \n","    def forward(self, x):\n","        x = self.layer_1(x) \n","        x = self.layer_2(x)\n","        x = self.layer_3(x)\n","        x = self.layer_4(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.layer_5(x)\n","    \n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tkWd1GkUKbc4","colab":{}},"source":["def call_RCNN(seq_len, batch_size, cv1_k = 1, cv1_s = 1, cv2_k = 1, cv2_s = 1,\n","              cv3_k = 1, cv3_s = 1, cv4_k = 3, cv4_s = 3):\n","    \n","    rcnn = RCNN(input_size = seq_len, batch_size = batch_size, n_features = 1, \n","                   cv1_k = cv1_k, cv1_s = cv1_s,\n","                   cv2_k = cv2_k, cv2_s = cv2_s,\n","                   cv3_k = cv3_k, cv3_s = cv3_s,\n","                   cv4_k = cv4_k, cv4_s = cv4_s)\n","\n","    rcnn = rcnn.cuda()\n","    \n","    return rcnn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K_ZmJv6fSrh6"},"source":["### Optimiser and Loss functions and Train/Test Split"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0fdQsPDeSr92","colab":{}},"source":["def dataloaders(rcnn, ppg, target, batch_size):\n","    optimizer = torch.optim.SGD(rcnn.parameters(), lr=0.001)\n","    loss_func = nn.MSELoss() # mean squared loss for regression\n","    loss_func = loss_func.cuda() # may need to check if cuda() available\n","\n","    x = torch.from_numpy(ppg)\n","    y = torch.from_numpy(target)\n","    \n","    x, y = Variable(x), Variable(y) # torch trains on Variable, so convert.\n","\n","    #DataLoader\n","    dataset_loader = DataLoader(x, batch_size=batch_size, shuffle=True)\n","    \n","    # Creat list of data and targets\n","    data = []\n","    for i in range(len(x)):\n","        data.append([x[i], y[i]])\n","\n","    num_batches = len(ppg)//batch_size \n","    # test to be 10% of data and train to be the rest\n","    test_percent = int(num_batches*0.1)\n","    test_split = batch_size*test_percent\n","    train_split = (len(data) - (test_split))\n","\n","    print(test_split)\n","    print(train_split)\n","\n","    train_dataset,test_dataset= torch.utils.data.random_split(data,(train_split, test_split))\n","\n","    trainloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n","    testloader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=batch_size)\n","    \n","    return optimizer, loss_func, trainloader, testloader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_PX4QBKTS1gg"},"source":["## Train RCNN"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MkQtukG5hGXE","colab":{}},"source":["def train(epochs, batch_size, seq_len, rcnn, trainloader, optimizer, loss_func):\n","    num_epochs = epochs\n","\n","    total_loss = []\n","    for epoch in range(num_epochs):  # loop over the dataset multiple times\n","        running_loss = 0.0\n","        \n","        for i, data in enumerate(trainloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            inputs = inputs.view(batch_size, 1, seq_len)\n","            labels = labels[:,None]\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = rcnn(inputs)\n","            loss = loss_func(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","        \n","            if i % 10 == 9:    # print every 10 mini-batches\n","                print('Epoch:[%d / %d], Batch: [%d],  Loss: %.3f' %\n","                      (epoch + 1, num_epochs, i + 1, running_loss / 10))\n","                total_loss.append(running_loss/10)\n","                running_loss = 0.0\n","\n","    print('Finished Training...')\n","    \n","    return rcnn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K24WXC5CmUOl"},"source":["## Test RCNN for HR Error Rate"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"i9G83iyfnYFU","colab":{}},"source":["def heart_rate_difference(labels, predictions, max_y):\n","    labels.shape\n","    predictions.shape\n","    difference = []\n","    \n","    for i in range(len(labels)):\n","        target = labels[i].detach().cpu().numpy()\n","        guess = predictions[i].detach().cpu().numpy()\n","\n","        target = np.around(target*max_y)\n","        guess = np.around(guess*max_y)\n","        difference.append(abs(target-guess) / target)\n","\n","    d = np.asarray(difference)\n","    d = np.mean(d)\n","  \n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zCIqxMvYmXst","colab":{}},"source":["def test_rcnn(batch_size, seq_len, rcnn, testloader, max_y):\n","    hre = []\n","    for i, data in enumerate(testloader, 0): #test_Loader\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","      \n","        inputs = inputs.view(batch_size, 1, seq_len)\n","        labels = labels[:,None]\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","        \n","        # forward + backward + optimize\n","        predictions = rcnn(inputs)    \n","        difference = heart_rate_difference(labels, predictions, max_y)\n","        hre.append(difference*100)\n","    return np.mean(hre)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRjSe0LO47Wb","colab_type":"text"},"source":["## Main - function calls\n","\n","When you call the RCNN function you can specify Conv-Pooling params which will affect the outcome of your heart rate error. \\\\\n","Your choice of conv-pooling ```filter (cv_k)``` and ```stride (cv_k)``` sizes will be dependent on  ```seq_len``` that changes with you downsampling factors ```dwns_factor```.  You can set these in the ```call_RCNN()``` function\\\\\n","\n","The results will be written to a json file in format: \\\\\n","[```batch_size```, ```exercise```, ```downsampled frequency```, ```heart rate error```]"]},{"cell_type":"code","metadata":{"id":"qEL-6eUxepd9","colab_type":"code","colab":{}},"source":["# Exercises in dataset\n","exercise = ['high', 'low', 'run', 'walk']\n","# Original sampling frequency\n","fs = 256.0\n","# Downsampling Factor\n","dwns_factor = [fs//256.0, fs//30.0, fs//15.0, fs//10.0, fs//5.0]\n","epochs = 50\n","# File Directory for data\n","fileDir='/content/drive/My Drive/WristSense_Experiments/Data/wrist'\n","\n","downsample=True\n","error_msg = []\n","\n","for exer in exercise:\n","    for d in dwns_factor:\n","        # Load Data\n","        PPG, ECG = load_data(fileDir, exer)\n","        # Preprocess Data\n","        d = int(d)\n","\n","        if d == 1:\n","          downsample == False\n","        else:\n","          downsample == True\n","        \n","        ppg, ecg = preprocess_PPG_ECG(PPG, ECG, downsample=downsample, ds_factor=d)\n","\n","        # Fix batching as dataset is not balanced\n","        # Can choose any of the values in the comments - this will affect results\n","\n","        if exer == 'high':  # 28, 30, 35, 36, 42, 45\n","          batch_size = 28 \n","        elif exer == 'low': # 34, 51\n","          batch_size = 34\n","        elif exer == 'run': # 26, 28, 52, 56\n","          batch_size = 28\n","        else:               # 37, 59\n","          batch_size = 37\n","\n","\n","        seq_len = len(ppg[0,:])\n","\n","        # Get ECG HR Ground Truth\n","        ecg_groundTruth, max_y =  gt_ECG(ecg)\n","\n","        # Call RCNN Model\n","        # You can specify Conv-Pooling params in call\n","        # Your choice should be dependent on  seq_len\n","        rcnn = call_RCNN(seq_len, batch_size)\n","\n","        # Set up data into train/test splits with targets\n","        optimizer, loss_func, trainloader, testloader = dataloaders(rcnn, ppg, ecg_groundTruth, batch_size)\n","        # Train the model\n","        trained_rcnn =  train(epochs, batch_size, seq_len, rcnn, trainloader, optimizer, loss_func)\n","        # Test the model/estimate HRE\n","        error = test_rcnn(batch_size, seq_len, trained_rcnn, testloader, max_y)\n","        print(\"Error: \" +str(error))\n","\n","        error_msg.append([\"batch_size: \"+str(batch_size)+ \", exercise: \" +str(exer) + \", frequency: \" + str(fs//d) + \"Hz, error: \" +str(error)])\n","\n","json = js.dumps(error_msg)\n","f = open('./errors.json','w')\n","f.write(json)\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"68gvuaFWzjCO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
