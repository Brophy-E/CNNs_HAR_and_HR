{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HAR.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BsUeGAXz8SKp","colab_type":"text"},"source":["# CNNs for Heart Rate Estimation and Human Activity Recognition in Wrist Worn Sensing Applications\n","\n","\n","This is code for reproducing the HAR results shown in the paper presented at the WristSense workshop as part of PerCom 2020."]},{"cell_type":"markdown","metadata":{"id":"t3v4J3nK8nNL","colab_type":"text"},"source":["## Data Collection"]},{"cell_type":"markdown","metadata":{"id":"vKOGRCqk8ou6","colab_type":"text"},"source":["The data was collected by [D. Jarchi and A. Casson (2017)](https://www.mdpi.com/2306-5729/2/1/1) and downloaded from [PhysioNet](https://physionet.org/content/wrist/1.0.0/)."]},{"cell_type":"markdown","metadata":{"id":"KTIFWmdU8rrI","colab_type":"text"},"source":["### If using Google Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"mNvD2Y4U8uQC","colab_type":"text"},"source":["You can run this notebook on Colab using the following cell to mount your drive and install some dependencies"]},{"cell_type":"code","metadata":{"id":"D3MyBjXB-mU6","colab_type":"code","outputId":"22ca25e0-7a3a-489a-fc10-bb48985b4edc","executionInfo":{"status":"ok","timestamp":1573506809599,"user_tz":0,"elapsed":969,"user":{"displayName":"Eoin Brophy","photoUrl":"","userId":"11479295761519532451"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","# This will prompt for authorization.\n","drive.mount('/content/drive')\n","\n","# Pull the data from PhysioNet - Specify a Path\n","#!gsutil -m cp -r gs://wrist-1.0.0.physionet.org '/content/drive/My Drive/..../Data'\n","\n","# Install wfdb\n","!pip install wfdb\n","\n","# Change cwd if necessary\n","import os\n","path = '/content/drive/My Drive/.../Data'\n","os.chdir(path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dIfx5LHR804C","colab_type":"text"},"source":["### If your running on your own machine/server\n","You may need to install some of these packages below"]},{"cell_type":"code","metadata":{"id":"mbx0ElcYuooC","colab_type":"code","colab":{}},"source":["import wfdb\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zt8OM0QW9s_m","colab_type":"text"},"source":["## Load Data\n","\n","This step is done in by selecting each exercise at a time. We can begin with the 'walk' exercise.\n","\n","By changing the word below betwwen 'high', 'low', 'run', 'walk' we can pre-process our data."]},{"cell_type":"code","metadata":{"id":"nhlDc9Iw9zJW","colab_type":"code","colab":{}},"source":["def load_data(fileDir, exercise):\n","    word = exercise\n","    file_path_list = []\n","    valid_file_extensions = [\".dat\"]\n","    valid_file_extensions = [item.lower() for item in valid_file_extensions]\n","\n","\n","    for file in os.listdir(fileDir):\n","        extension = os.path.splitext(file)[1]\n","        if extension.lower() not in valid_file_extensions:\n","            continue\n","        file_path_list.append(os.path.join(fileDir, file))\n","\n","    PPG = []\n","    for path in file_path_list:\n","        base=os.path.basename(path)\n","        base = os.path.splitext(base)[0]\n","        if word in base:\n","            sample = wfdb.rdsamp('wrist/%s'%(base))\n","            PPG.append(sample[0][:,1])\n","\n","    PPG = np.asarray(PPG)\n","    return PPG"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KlCLUdFu0Ceg","colab_type":"text"},"source":["## Segment Data\n","\n","\n","```slidingWindow()``` returns a generator that iterates through the input sequence."]},{"cell_type":"code","metadata":{"id":"QPOYva5w223T","colab_type":"code","colab":{}},"source":["def slidingWindow(sequence,winSize=2048,step=256):\n","    \"\"\"Returns a generator that will iterate through\n","    the defined chunks of input sequence.  Input sequence\n","    must be iterable.\"\"\"\n"," \n","    # Verify the inputs\n","    try: it = iter(sequence)\n","    except TypeError:\n","        raise Exception(\"**ERROR** sequence must be iterable.\")\n","    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n","        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n","    if step > winSize:\n","        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n","    if winSize > len(sequence):\n","        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n"," \n","    # Pre-compute number of chunks to emit\n","    numOfChunks = ((len(sequence)-winSize)//step)+1\n"," \n","    # Do the work\n","    for i in range(0,numOfChunks*step,step):\n","        yield sequence[i:i+winSize]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"atMSFxrxF7qX","colab_type":"text"},"source":["### PPG\n","\n","Here the PPG signal is segmented using the ```slidingWindow``` function\n","\n","The returned signal **p** is ready to be plotted"]},{"cell_type":"code","metadata":{"id":"4OU1tvpC6R7n","colab_type":"code","outputId":"d649f741-94eb-44fe-c4a4-2539ae8cbd98","executionInfo":{"status":"ok","timestamp":1573507553986,"user_tz":0,"elapsed":7479,"user":{"displayName":"Eoin Brophy","photoUrl":"","userId":"11479295761519532451"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def segment_PPG(PPG):\n","  p = []\n","\n","  for i in range(len(PPG)):\n","    ppg = slidingWindow(PPG[i], winSize = 2048, step = 256)\n","    for j in ppg:\n","      p.append(j)\n","    \n","  p = np.asarray(p)\n","  return p"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(2190, 2048)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oL7SmNPtF_GV","colab_type":"text"},"source":["##Plot and Save PPG"]},{"cell_type":"code","metadata":{"id":"RxotMr9FCIap","colab_type":"code","colab":{}},"source":["def plot_and_save(exercise, p, ds_factor):\n","  pixel_size = 386\n","  my_dpi = 96\n","  fs = 256.0\n","\n","  for i in range(len(p)):\n","    fig = plt.figure( figsize = (pixel_size/my_dpi, pixel_size/my_dpi), dpi=my_dpi )\n","    ax = fig.add_subplot(111)\n","    ax.plot(p[i, ::ds_factor])\n","    # Bound the plot to prevent normalising of signals\n","    ax.scatter(0, -30, c='w') # min value of all signals\n","    ax.scatter(0, 2820, c='w') # max value of all signals\n","\n","    # Remove all extraneous graph elements\n","    ax.axes.get_xaxis().set_visible(False)\n","    ax.axes.get_yaxis().set_visible(False)\n","    ax.set_frame_on(False)\n","\n","    # Save the figure and close\n","    plt.savefig('/content/drive/My Drive/.../HAR/PPG_'+str(int(fs//ds_factor))+'Hz/'+\n","                str(exercise)+'/'+str(i)+'.jpg', bbox_inches = 'tight', pad_inches = 0)\n","    plt.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88DgaVKV-exm","colab_type":"text"},"source":["## Main - function calls\n","\n","The functions above are called to segment, plot and save your PPG data into folders ready to be used in Transfer Learning for HAR"]},{"cell_type":"code","metadata":{"id":"VyIbtBv--fY9","colab_type":"code","colab":{}},"source":["# Exercises in dataset\n","exercise = ['high', 'low', 'run', 'walk']\n","# Original sampling frequency\n","fs = 256.0\n","# Downsampling Factor\n","# 256Hz 30Hz 15Hz 12Hz 11Hz 10Hz 9Hz 8Hz 5Hz 1Hz\n","dwns_factor = [fs//256.0, fs//30.0, fs//15.0, fs//12.0, fs/11.0, fs//10.0, fs//9.0, fs//8.0, fs//5.0 fs//1.0]\n","# File Directory for data\n","fileDir='/content/drive/My Drive/.../Data/wrist'\n","\n","\n","for exer in exercise:\n","    # Load Data - only need to load once per exercise\n","    PPG = load_data(fileDir, exer)\n","    # Downsample and plot\n","    for d in dwns_factor:\n","        d = int(d)\n","        # Segment data\n","        ppg = segment_PPG(PPG)\n","        # Plot and save - ready for Transfer Learning\n","        plot_and_save(exer, ppg, d):"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Isz0oa_gKpdR","colab_type":"text"},"source":["# Tranfer Learning"]},{"cell_type":"markdown","metadata":{"id":"8ygVySofMZ69","colab_type":"text"},"source":["## Script"]},{"cell_type":"markdown","metadata":{"id":"cVXouX6cKxLE","colab_type":"text"},"source":["After plotting and formatting of your PPG data you can run the Transfer Learning script that was taken from [Tensorflow for Poets](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0). \\\\\n","\n","There is now a simplified notebook available on Google Colaboratory [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning_with_hub.ipynb). \\\\\n","\n","***Note***: We used the transfer learning script from Tensorflow 1.0, which is now deprecated. The results from the new scripts using Tensorflow 2.0 may vary slightly."]},{"cell_type":"markdown","metadata":{"id":"RKGa9sFMMb8_","colab_type":"text"},"source":["## Parameters"]},{"cell_type":"markdown","metadata":{"id":"RRU0mJo1MGez","colab_type":"text"},"source":["All default parameter values bar one were kept the same.\n","\n","We changed the number of training iterations from a default value of 10,000 to 4,000, this helped minimise overfitting through sufficient convergence of the loss function"]}]}